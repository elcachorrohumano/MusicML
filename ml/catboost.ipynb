{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureEngineer:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        self.pca = PCA(n_components=0.95)\n",
    "        self.selector = SelectKBest(f_classif, k='all')\n",
    "    \n",
    "\n",
    "    def create_interaction_features(self, X):\n",
    "        num_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "        interactions = pd.DataFrame()\n",
    "        \n",
    "        for i, col1 in enumerate(num_cols):\n",
    "            for col2 in num_cols[i+1:]:\n",
    "                interactions[f'{col1}_{col2}_mult'] = X[col1] * X[col2]\n",
    "                interactions[f'{col1}_{col2}_div'] = X[col1] / (X[col2] + 1e-8)\n",
    "                \n",
    "        return interactions\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        interactions = self.create_interaction_features(X)\n",
    "        X_combined = pd.concat([X, interactions], axis=1)\n",
    "        \n",
    "        X_scaled = self.scaler.fit_transform(X_combined)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X_combined.columns)\n",
    "        \n",
    "        X_poly = self.poly.fit_transform(X_scaled)\n",
    "        poly_features = pd.DataFrame(X_poly, columns=[f'poly_{i}' for i in range(X_poly.shape[1])])\n",
    "        \n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "        pca_features = pd.DataFrame(X_pca, columns=[f'pca_{i}' for i in range(X_pca.shape[1])])\n",
    "        \n",
    "        final_features = pd.concat([X_scaled, poly_features, pca_features], axis=1)\n",
    "        \n",
    "        self.selector.fit(final_features, y)\n",
    "        selected_mask = self.selector.get_support()\n",
    "        selected_features = final_features.iloc[:, selected_mask]\n",
    "        self.feature_names = selected_features.columns.tolist()\n",
    "        \n",
    "        return selected_features\n",
    "\n",
    "    def transform(self, X):\n",
    "        interactions = self.create_interaction_features(X)\n",
    "        X_combined = pd.concat([X, interactions], axis=1)\n",
    "        \n",
    "        X_scaled = self.scaler.transform(X_combined)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=X_combined.columns)\n",
    "        \n",
    "        X_poly = self.poly.transform(X_scaled)\n",
    "        poly_features = pd.DataFrame(X_poly, columns=[f'poly_{i}' for i in range(X_poly.shape[1])])\n",
    "        \n",
    "        X_pca = self.pca.transform(X_scaled)\n",
    "        pca_features = pd.DataFrame(X_pca, columns=[f'pca_{i}' for i in range(X_pca.shape[1])])\n",
    "        \n",
    "        final_features = pd.concat([X_scaled, poly_features, pca_features], axis=1)\n",
    "        return final_features[self.feature_names]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob)\n",
    "    }\n",
    "\n",
    "def train_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = evaluate_model(y_test, y_pred, y_prob)\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return model, metrics\n",
    "\n",
    "def fine_tune_lgbm(X_train, y_train, X_test, y_test):\n",
    "    model = LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        feature_fraction=0.8\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # best_iteration = model.best_iteration_\n",
    "    # final_model = LGBMClassifier(\n",
    "    #     n_estimators=best_iteration,\n",
    "    #     learning_rate=0.05,\n",
    "    #     num_leaves=min(31, int(best_iteration/5)),\n",
    "    #     feature_fraction=0.7,\n",
    "    #     bagging_fraction=0.8,\n",
    "    #     bagging_freq=5\n",
    "    # )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ml_pipeline(X_train, X_test, y_train, y_test):\n",
    "    # Feature engineering\n",
    "    fe = FeatureEngineer()\n",
    "    X_train_engineered = fe.fit_transform(X_train, y_train)\n",
    "    X_test_engineered = fe.transform(X_test)\n",
    "    \n",
    "    # 1. CatBoost with cross-validation\n",
    "    catboost_params = {\n",
    "        'iterations': 1000,\n",
    "        'learning_rate': 0.1,\n",
    "        'depth': 6,\n",
    "        'l2_leaf_reg': 3,\n",
    "        'eval_metric': 'AUC',\n",
    "        'verbose': False\n",
    "    }\n",
    "    \n",
    "    catboost = CatBoostClassifier(**catboost_params)\n",
    "    cv_scores = cross_validate(catboost, X_train_engineered, y_train, \n",
    "                             cv=5, scoring=['accuracy', 'precision', 'recall', 'f1'])\n",
    "    \n",
    "    print(\"\\nCatBoost Cross-Validation Results:\")\n",
    "    for metric, scores in cv_scores.items():\n",
    "        if metric.startswith('test'):\n",
    "            print(f\"{metric}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    catboost_model, catboost_metrics = train_evaluate_model(\n",
    "        catboost, X_train_engineered, X_test_engineered, y_train, y_test, \"CatBoost\"\n",
    "    )\n",
    "    \n",
    "    # 2. XGBoost with GridSearch\n",
    "    xgb_param_grid = {\n",
    "        'max_depth': [3],\n",
    "        'learning_rate': [0.01],\n",
    "        'n_estimators': [100],\n",
    "        'subsample': [0.8]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    grid_search = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train_engineered, y_train)\n",
    "    \n",
    "    print(\"\\nXGBoost Best Parameters:\", grid_search.best_params_)\n",
    "    xgb_model, xgb_metrics = train_evaluate_model(\n",
    "        grid_search.best_estimator_, X_train_engineered, X_test_engineered, \n",
    "        y_train, y_test, \"XGBoost\"\n",
    "    )\n",
    "    \n",
    "    # 3. LightGBM with custom fine-tuning\n",
    "    lgbm_model = fine_tune_lgbm(X_train_engineered, y_train, X_test_engineered, y_test)\n",
    "    lgbm_model, lgbm_metrics = train_evaluate_model(\n",
    "        lgbm_model, X_train_engineered, X_test_engineered, y_train, y_test, \"LightGBM\"\n",
    "    )\n",
    "    \n",
    "    # Compare models\n",
    "    all_metrics = {\n",
    "        'CatBoost': catboost_metrics,\n",
    "        'XGBoost': xgb_metrics,\n",
    "        'LightGBM': lgbm_metrics\n",
    "    }\n",
    "    \n",
    "    metrics_df = pd.DataFrame(all_metrics).round(4)\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # Return best model\n",
    "    best_model_name = metrics_df.loc['roc_auc'].idxmax()\n",
    "    best_model = {\n",
    "        'CatBoost': catboost_model,\n",
    "        'XGBoost': xgb_model,\n",
    "        'LightGBM': lgbm_model\n",
    "    }[best_model_name]\n",
    "    \n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    return best_model, fe, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train = pd.read_csv('../data/train/train.csv')\n",
    "test = pd.read_csv('../data/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['like']\n",
    "X_train = train.drop(columns=['like'])\n",
    "\n",
    "y_test = test['like']\n",
    "X_test = test.drop(columns=['like'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model, feature_engineer, metrics_df = run_ml_pipeline(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
